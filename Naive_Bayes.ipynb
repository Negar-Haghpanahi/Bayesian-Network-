{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "_fADt8EfOQas"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "from re import findall\n",
    "from hazm import Stemmer, word_tokenize\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from glob import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import time\n",
    "\n",
    "\"\"\"Countvectorzer initialize\"\"\"\n",
    "vect = CountVectorizer()\n",
    "is_file = True\n",
    "\n",
    "\"\"\"stemmer from hazm\"\"\"\n",
    "stemmer = Stemmer()\n",
    "stopwords = set(open('stop-words.txt', encoding='utf8').read().splitlines())\n",
    "hamtrains = glob('dataset/hamtraining/hamtraining*.txt')\n",
    "spamtrains = glob('dataset/spamtraining/spamtraining*.txt')\n",
    "\n",
    "hamtests = glob('dataset/hamtesting/hamtesting*.txt')\n",
    "spamtests = glob('dataset/spamtesting/spamtesting*.txt')\n",
    "\n",
    "ham_spam_test = hamtests + spamtests\n",
    "ham_spam_train = hamtrains + spamtrains\n",
    "\n",
    "y_train = [False for i in range(300)]\n",
    "y_spam_train = [True for i in range(300)]\n",
    "y_train.extend(y_spam_train)\n",
    "\n",
    "\n",
    "y_test = [False for i in range(200)]\n",
    "y_spam_test = [True for i in range(200)]\n",
    "y_test.extend(y_spam_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in f:\\anacoda\\lib\\site-packages (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in f:\\anacoda\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in f:\\anacoda\\lib\\site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in f:\\anacoda\\lib\\site-packages (from nltk==3.3->hazm) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install hazm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkAXHzrkL1S5"
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "TLo8Z3tjNY6A"
   },
   "outputs": [],
   "source": [
    "class const:\n",
    "    farsi = ('ب', 'س', 'ش', 'ل', 'ت', 'ن', 'م', 'گ', 'ظ', 'ط', 'ز',\n",
    "             'ر', 'ژ', 'ذ', 'د', 'پ', 'چ', 'ج', 'ح', 'ع', \n",
    "             'خ', 'غ', 'ف', 'ق', 'ث', 'ص', 'ض','\\u0020', '\\u060c','؟', '!', '?', '.', ':','\\n','_')\n",
    "\n",
    "    alef = ('ا', 'آ', 'ء', 'أ', 'إ')\n",
    "    vav = ('و', 'ؤ')\n",
    "    heh = ('ه', 'ة', 'ە')\n",
    "    yah = ('ی', 'ي', 'ئ', 'ى')\n",
    "    kaf = ('ک', 'ك')\n",
    "    punc = ('_', '-')\n",
    "\n",
    "def persian_char(char):\n",
    "    if char in const.farsi:\n",
    "        return char\n",
    "    if char in const.alef:\n",
    "        return const.alef[0]\n",
    "    if char in const.vav:\n",
    "        return const.vav[0]\n",
    "    if char in const.heh:\n",
    "        return const.heh[0]\n",
    "    if char in const.yah:\n",
    "        return const.yah[0]\n",
    "    if char in const.kaf:\n",
    "        return const.kaf[0]\n",
    "    if char in const.punc:\n",
    "      return ' '\n",
    "    return ''\n",
    "\n",
    "def pre_process(path):\n",
    "    if is_file == False:\n",
    "        text = path\n",
    "    else:\n",
    "        text = open(path, encoding='utf8').read()\n",
    "\n",
    "#     urls = len(findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', text))\n",
    "    \n",
    "    map_test = map(persian_char, text)\n",
    "    sentence = ''.join(map_test)\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    filtered_sentence = [w for w in word_tokens if w not in stopwords] #+ ['url' for i in range(urls)]\n",
    "    filtered_sentence = ' '.join(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "eq6fmbKJNAON"
   },
   "outputs": [],
   "source": [
    "def feature(data):\n",
    "    if is_file == False:\n",
    "        feature_list = [pre_process(data)]\n",
    "    else:    \n",
    "        map_loop = map(pre_process, data)\n",
    "        feature_list = list(map_loop)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "f4W7WDyHSIqJ"
   },
   "outputs": [],
   "source": [
    "def vectorize(feature_list):\n",
    "    X_dtm = vect.fit_transform(feature_list)\n",
    "    X_dtm = X_dtm.toarray()\n",
    "    return X_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "EctJShjtRrtx"
   },
   "outputs": [],
   "source": [
    "def feature_selection(k,X_dtm):\n",
    "    chi2_features = SelectKBest(chi2, k=k)\n",
    "    X_kbest_features = chi2_features.fit_transform(X_dtm, y_train)\n",
    "    return X_kbest_features, chi2_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "kW63H6H2M081"
   },
   "outputs": [],
   "source": [
    "def transform(data,chi2_features):\n",
    "    tokens = feature(data)\n",
    "    x0 = vect.transform(tokens).toarray()\n",
    "    cx = chi2_features.transform(x0)\n",
    "    return cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "QG_JW7OCRv33"
   },
   "outputs": [],
   "source": [
    "X_dtm = vectorize(feature(ham_spam_train))\n",
    "x_train, chi2_features = feature_selection(700,X_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So9qhaNiL6k2"
   },
   "source": [
    "### Just for Intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KVb5YkTRxbH",
    "outputId": "34bd04b5-0539-4a67-decd-4a73cbae81a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "do9ij0utRzKN"
   },
   "outputs": [],
   "source": [
    "\"\"\"Just for test and compare\"\"\"\n",
    "x_test = transform(ham_spam_test, chi2_features)\n",
    "predict_val = mnb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqVRo_9cR1UB",
    "outputId": "91fade46-9687-4d17-8678-67d74eb70ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.97      0.95       200\n",
      "        True       0.97      0.93      0.95       200\n",
      "\n",
      "    accuracy                           0.95       400\n",
      "   macro avg       0.95      0.95      0.95       400\n",
      "weighted avg       0.95      0.95      0.95       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Show metrics and score!\"\"\"\n",
    "print(classification_report(y_test, predict_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T_yx4pSNBbK"
   },
   "source": [
    "### Naive bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgA5NkyZViH-"
   },
   "source": [
    "<h2 dir=\"rtl\"> ویدیو بسیار مفید برای ساخت مدل<h2>\n",
    "<h2><a href=\"https://www.youtube.com/watch?v=O2L2Uv9pdDA\">Naive Bayes, Clearly Explained!!!</a><h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mkQImpgGIUK"
   },
   "source": [
    "<h3 dir=\"rtl\">خرجی تابع\n",
    "transform \n",
    "وکتورایز شده هر یک از سمپل‌های موجود در دیتاست تست است.\n",
    "به طوری که تعداد اعضای \n",
    "test_data\n",
    "برابر با 400 نمونه متنی است که برای تست کردن مدل مورد استفاده قرار میگیرد.\n",
    "هر یک از سمپل‌ها دارای وکتوری به طول 700 میباشد، که تعداد فیچرهای(کلمات) برتر انتخاب شده توسط تابع SelectKBest \n",
    "اند.\n",
    "\n",
    "در این وکتور 700 بعدی مقدار هر یک از اندیس‌ها نشان دهنده تکرار آن کلمه در آن سمپل خاص هست.\n",
    "\n",
    "راهنمایی ساخت جدول احتمالات:\n",
    "شما با کمک تکرار کلمات در هر سمپل و اینکه هر کلمه در کل نمونه‌های \n",
    "spam\n",
    "یا\n",
    "ham\n",
    "چند بار تکرار شده است میتوانید احتمال رخدادن هر کلمه را در هر دسته بدست آورید\n",
    "که در نهایت با ضرب احتمالات بدست آمده تعیین میکنید که ایمیل \n",
    "spam\n",
    "است یا نه.\n",
    "\n",
    "به این شکل که بعد از ضرب احتمالات در هم دیگر؛ اگر مقدار احتمال دسته \n",
    "spam \n",
    "بیشتر از دسته \n",
    "ham(nonspam)\n",
    "بود،\n",
    "آنگاه آن ایمیل spam \n",
    "است.\n",
    "<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "5PWMEKHsNhaS"
   },
   "outputs": [],
   "source": [
    "test_data = transform(ham_spam_test, chi2_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vVys7IZJLUJ",
    "outputId": "a7acc491-9ac2-44f8-b745-f2e221a2bd6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CItWMrA3JqAY"
   },
   "source": [
    "<h3 dir=\"rtl\"> \n",
    "شما باید با کمک این فیچر‌ها مدل \n",
    "naive bayes \n",
    "را برای تشخیص ایمیل‌های \n",
    "spam, nonspam\n",
    "آماده سازی کنید.\n",
    "خروجی کار شما باید لیبل‌های باشد که تعیین میکند،\n",
    "ایمیل \n",
    "spam هست\n",
    "یا نه.\n",
    "\n",
    "<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yApjdXwLtOG"
   },
   "source": [
    "<h1>Please implement your naive bayse model in here.<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "VDLFHZYZLWdr"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "#Build your naive bayes classifier!\n",
    "def bayes(test_data):\n",
    "\n",
    "    spam_vec=np.array(test_data)\n",
    "    ham_vec=np.array(test_data)         #now we have to divide each word into the total\n",
    "                                    #  number of   spam words and do the same for ham words   \n",
    "    for i in range(0,len(test_data)):\n",
    "        spam_vec[i]=test_data[i]/x_train  \n",
    "        ham_vec[i]=test_data[i]/y_train\n",
    "\n",
    "    multi_spam = 1                  #it is better to use log for every answer \n",
    "                                    # to prevent overflow (because of multiplying each probability)\n",
    "                                    # and use exp for final answer\n",
    "    for x in range(0,spam_vec):\n",
    "            multi= math.log( multi) * math.log( x)\n",
    "\n",
    "    multi_ham=1\n",
    "    for x in range(0,ham_vec):\n",
    "            multi_ham= math.log(multi_ham) *math.log( x)\n",
    "\n",
    "    #the probability of being spam:\n",
    "    spam=math.exp((300/600)*multi_spam)\n",
    "    ham=math.exp((300/600)*multi_ham)\n",
    "\n",
    "    if spam > ham:\n",
    "        print(\"this a spam\")\n",
    "    else:\n",
    "        print(\"this a ham\")\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "yY8f_1_rFJSA"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### \n",
    "bayes(test_data)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2dZ-dvANM1n"
   },
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSBVMGRAMUX-",
    "outputId": "ffc3c198-b838-4988-c41a-d6194388a441"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\NEGAR UNI\\term 6\\AI\\6\\Naive_Bayes.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/NEGAR%20UNI/term%206/AI/6/Naive_Bayes.ipynb#ch0000022?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"Show metrics and score!\"\"\"\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/NEGAR%20UNI/term%206/AI/6/Naive_Bayes.ipynb#ch0000022?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test, y_predict))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_predict' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Show metrics and score!\"\"\"\n",
    "print(classification_report(y_test, y_predict))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive Bayes.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "9a164ba7025457f1372271eff055a5f1d8f80807a08e5139340cb124d314090e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
